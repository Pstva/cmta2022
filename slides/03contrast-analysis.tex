% много редких events
%Log-likelihood tests
%The practical effect of this improvement is that
%statistical textual analysis can be done effectively with very much smaller volumes of
%text than is necessary for conventional tests based on assumed normal distributions,
%and it allows comparisons to be made between the significance of the occurrences of
%both rare and common phenomenon.
%
%
%The likelihood ratio for a hypothesis is the ratio of the maximum value of the likelihood
%function over the subspace represented by the hypothesis to the maximum value of
%the likelihood function over the entire parameter space.
%
%
%The particularly important feature of likelihood ratios is that the quantity -2 log )~
%is asymptotically X 2 distributed with degrees of freedom equal to the difference in
%dimension between f~ and f~0. Importantly, this asymptote is approached very quickly
%in the case of binomial and multinomial distributions.
%
%If the null hypothesis holds, then the log-likelihood ratio is asymptotically X 2 distributed with k/2 - 1 degrees of freedom. When j is 2 (the binomial), -2 log )~ will be
%X 2 distributed with one degree of freedom.
%
%The likelihood ratio -> the logarithm of the likelihood ratio ->
%
%https://ucrel.lancs.ac.uk/llwizard.html


\documentclass[svgnames]{beamer}
%\mode<presentation>
%{
%  \usetheme[titleformat=smallcaps,numbering=fraction,progressbar=frametitle]{metropolis}
%  \usecolortheme[light,accent=orange]{solarized}
%  %\usecolortheme[named=Goldenrod]{structure}
%  % or ...
%
%  \setbeamercovered{transparent}
%  % or whatever (possibly just delete it)
%}


% \usepackage{mathtext}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{cmap}
\hypersetup{unicode=true}
\graphicspath{{images/}{slides/images}}


\title[CMTA 03] % (optional, use only with long paper titles)
{Contrastive analysis}

\subtitle
{Computational Methods for Text Analysis} % (optional)

\author%[Author, Another] % (optional, use only with lots of authors)
{Pestova Alena Sergeevna}
% - Use the \inst{?} command only if the authors have different
%   affiliation.

\institute%[Universities of Somewhere and Elsewhere] % (optional, but mostly needed)
{НИУ ВШЭ Санкт-Петербург}
% - Use the \inst command only if there are several affiliations.
% - Keep it simple, no one is interested in your street address.

\date%[Short Occasion] % (optional)
{25.09.2021 / 03}

\subject{natural language processing, text mining}
% This is only inserted into the PDF information catalog. Can be left
% out. 


%\AtBeginSubsection[]
%{
%  \begin{frame}<beamer>[plain]{План}
%    \tableofcontents[sectionstyle=show/hide,subsectionstyle=show/shaded/hide]
%  \end{frame}
%}

\newcommand{\tb}[1]{\colorbox{yellow}{#1}\space}
\newcommand{\Sp}[1]{\colorbox{green}{#1}\space}
\newcommand{\Sn}[1]{\colorbox{red}{#1}\space}


\begin{document}

\begin{frame}
  \titlepage
\end{frame}


\begin{frame}
  \frametitle{Corpus-based contrastive analysis}
  The task is to extract vocabulary specific to a given corpus
  \begin{itemize}
  \item \textbf{Reference corpus} represents word usage in a language in general or in some subject area
  \item Build the frequency lists for the corpus of interest and the reference corpus
  \item Sort words by the difference in frequency of the studied corpus with the reference corpus
  \item Keywords of the studied corpus are at the top of the list - the words that are more specific to this particular corpus
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Keywords of the corpus}
  \begin{block}{Simple maths (by Adam Kilgarriff)}
  «this word is twice as common in this corpus as in that corpus»
\end{block}
\begin{itemize}
\item The simplest way
  \begin{itemize}
  \item Normalize the frequencies
    \begin{itemize}
    \item metric Instances per million (IPM)
    \end{itemize}
  \item Calculate the ratio of the normalized frequencies
  \item Sort the lists by the calculated reatio
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
  
For example:
\begin{itemize}
\item Two corpora, the size of each corpus is one million tokens
\item We do not need to normalize frequencies
\item[Fc] focus corpus — the studies corpus
\item[Rc] reference corpus
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Problem 1:  we cannot divide by 0}
  \begin{tabular}[l]{lccc}
    word & fc & rc & ratio \\
    \hline
    rarity & 10 & 0 &  ? \\
    stir & 100 & 0 &  ? \\
    yummy & 1000 & 0 &  ? \\
  \end{tabular}
\end{frame}

\begin{frame}

Standard solution - add 1:
  \begin{tabular}[l]{lccc}
    word & fc & rc & ratio \\
    \hline
    rarity & 11 & 1 &  11 \\
    stir & 101 & 1 &  101 \\
    yummy & 1001 & 1 &  1001 \\
  \end{tabular}
\end{frame}

\begin{frame}
  \frametitle{Problem 2: there are two many big ratios because of rare words}
  The frequency is also important.   Solution: add n.

  \begin{itemize}
  \item $n=1$

  \begin{tabular}[l]{lcccccc}
    word & fc & rc & fc+n & rc+n & ratio & rank \\
    \hline
    rare & 10 & 0 & 11 & 1 & 11,00 & 1 \\
    sometimes & 200 & 100 & 201 & 101 & 1,99 & 2 \\
    frequent & 12000 & 10000 & 12001 & 10001 & 1,20 & 3 \\
  \end{tabular}
  
  \item $n=100$

  \begin{tabular}[l]{lcccccc}
    word & fc & rc & fc+n & rc+n & ratio & rank \\
    \hline
    rare & 200 & 300 & 300 & 400 & 0.75 & 3 \\
    sometimes & 10 & 0 & 110 & 100 & 1,10 &  2 \\
    frequent & 12000 & 10000 & 12100 & 10100 & 1,20 & 1 \\
  \end{tabular}

  \end{itemize}
  
\end{frame}


\begin{frame}
  \frametitle{Normality and words distribution}

In the paradigm of the standard statistical tests, there were problems with comparing frequiencies

  \begin{itemize}
  \item The normality assumption is unlikely in the case of words frequency distribution
  \item There are too many rare events in the language (remember Zipf's Law)
  \item Inapplicability of tests based on the assumption of normality (e.g. chi-square), at least to rare events (frequency < 5)
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Dunning log-likelihood: motivation}
  \framesubtitle{Log likelihood ratio}

A way to incorporate word frequencies into the statistical test paradigm:

  \href{https://aclanthology.org/J93-1003.pdf}{Ted Dunning "Precise
    Surprise and Coincidence Statistics Methods (1994)}
  \begin{itemize}
  \item Dunning log-likelihood is less dependent on an assumption of the distribution normality
  \item Therefore, it does not overestimate the detection of rare events so much and
    can be used for evaluation of not only the most frequent words
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Dunning log-likelihood: formulas}
  \begin{tabular}[c]{|p{.3\textwidth}|c|c|c|}
    \hline
   & Corpus 1 & Corpus 2 & Total \\
    \hline
    Word Frequency & a & b & a+b \\
    \hline
    Frequency of other words & c & d & c+d \\
    \hline
    Total & a+c & b+d & a+b+c+d \\
    \hline
  \end{tabular}

\bigskip
  Expected frequencies:
  \begin{itemize}
  \item[Eij] $\frac{R_iC_j}{N}$
  \item[E1] $\frac{(a+b)(a+c)}{(a+b+c+d)}$
  \item[E2] $\frac{(a+b)(b+d)}{(a+b+c+d)}$
  \item[E3] $\frac{(c+d)(a+c)}{(a+b+c+d)}$
  \item[E3] $\frac{(c+d)(b+d)}{(a+b+c+d)}$
  \end{itemize}

\end{frame}
\begin{frame}{Dunning log-likelihood: formulas}


  old:\\

  $LL = G^2 = 2 (a \log (a/E1) + b \log (b/E2)$ \\


  new (almost equal to the previous one):\\
  
  $LL = G^2 = 2 (a \log (a/E1) + b \log (b/E2) +c \log(c/E3) + d \log(d/E4))$



\end{frame}

\begin{frame}
  If we calculate the log-likelihood ratio test for two words in two corpora, then


    $$G^2 \approx X^2(1)$$


      We can calculate statistical significance of the difference (on the level of significance 0.05):

      $$\text{p-value}: P(X^2 >= 3.84)$$

      $$\text{CDF}(3.84) = 0.95 \text{for} X^2$$

\end{frame}

\begin{frame}
  \frametitle{Log-Likelihood ratio}

  \begin{itemize}
    \item The practical effect of this improvement is that  statistical textual analysis can be done
  effectively with very much smaller volumes of text than is necessary for conventional tests based on assumed normal distributions
  \item It allows comparisons to be made between the significance of the occurrences of both rare and common phenomenon.
  \item More sensitive to frequent events (words) than to less frequent one [underestimates the degree of difference for less frequent words]
  \end{itemize}
\end{frame}

\begin{frame}{Log-odds}

  $$\text{LR} = \log\frac{(a/sum(a))}{(b/sum(b))}$$

\end{frame}


\begin{frame}
  \begin{itemize}
    \item $G^2$ for stat. significance and log-odds for effect size, sorting by log-odds
    \item sorting by log-odds and/or $G^2$, finding a threshold for cutting lists
\end{itemize}
\end{frame}


\begin{frame}{Collocations}
  Collocations is a term that denotes pairs of words that not only stand
togther frequently, but their appearence next to each other is above
the mere chance. That means, there is statistically significant
association between these words.
\end{frame}

\begin{frame}
  \frametitle{Сonditional Probability}
$$
    P(Event|Condition)
$$
\end{frame}

\begin{frame}
  \frametitle{Сonditional Probability}
  \begin{equation}
    P(B|A) = \frac{P(B \land A)}{P(A)}
  \end{equation}

  $$
    P(\text{intelligence}|\text{artificial}) =
    \frac{P(\text{artificial intelligence})}{P(\text{artificial})}
    =$$

    $$
    = \frac{\frac{4}{46804371}}{\frac{121}{46804371}} = \frac{4}{121} = 0.033$$
\end{frame}

\begin{frame}
  \frametitle{Pointwise mutual information}
  \begin{columns}
    \column{.5\textwidth}
    PMI

  $$
  pmi(x;y) = log \frac{p(x,y)}{p(x)p(y)} =
  $$
  $$
  = log \frac{p(x|y)}{p(x)} =
  $$
  $$
  = log \frac{p(y|x)}{p(y)}
  $$

  Positive PMI (negative values do not make much sense for word association measurements)

    $$
    ppmi(x;y) = max(pmi(x;y),0)
    $$
    \column{.5\textwidth}
  \includegraphics[width=\textwidth]{log}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{The problem of PMI}
  \begin{itemize}
  \item Very sensitive to rare events that are highly informative relative to each other (words always occur together)
    [increase the degree of difference for rare words]
  \end{itemize}
\end{frame}

\begin{frame}
  PMI can be used:
  \begin{itemize}
    \item For extracting vocabulary specific to a given corpus (as well as G2, log-odds, frequency ratios)
    \item For finding collocations in the text (as well as G2 and log-odds)
  \end{itemize}
\end{frame}

\begin{frame}{Contrastive analysis}
  We have seen how to calculate metrics for contrastive analysis with g2 and log-odds. Let's use PMI for the same thing.
  \begin{tabular}[l]{lcc}
    word & fc & rc \\
    \hline
    sometimes & 200 & 100 \\
    other words & 10000 & 20000\\
  \end{tabular}

  \begin{itemize}
  \item $x$ - meeting the word "sometimes"
  \item $y$ - corpus 1 (size of the corpus)
  \item $p(x|y)= 200 / 10200 = 0.0196$ - probablity of meeting word "sometimes" in corpus 1
  \item $p(x)= 300 / (300+10000+20000) = 0.0099$ - probablity of meeting word "sometimes" in total (in corpus 1 and corpus 2 in our case)
  \end{itemize}
  
  $$pmi(x;y) = log \frac{p(x|y)}{p(x)} = log \frac{0.0196}{0.0099} = 0.9854$$
\end{frame}

\begin{frame}{Collocations}
  \begin{tabular}[l]{lccc}
    bigram & word1 freq & word2 freq & bigram freq \\
    \hline
    united states & 200 & 100 & 50\\
    all other bigrams & 10000 & 10000 & ...\\
  \end{tabular}
    \begin{itemize}
  \item $x$ - meeting the word "united" in the corpus
  \item $y$ - meeting the word "states" in the corpus
  \item $p(x)=200/10000 = 0.02$ - probability of meeting the word "united"
  \item $p(x|y) = 50/1000=0.05$ - probability of meeting the word "united" together with the word "states"
    \end{itemize}

  $$pmi(x;y) = log \frac{p(x|y)}{p(x)} $$

\end{frame}

\begin{frame}
  \frametitle{Dunning log-likelihood: formulas}
  \begin{tabular}[c]{|p{.3\textwidth}|c|c|c|}
    \hline
   & Word 2 & not Word 2 & Total \\
    \hline
    Word 1 & a & b & a+b \\
    \hline
    not Word 1 & c & d & c+d \\
    \hline
    Total & a+c & b+d & a+b+c+d \\
    \hline
  \end{tabular}

\bigskip
  Expected frequencies:
  \begin{itemize}
  \item[Eij] $\frac{R_iC_j}{N}$
  \item[E1] $\frac{(a+b)(a+c)}{(a+b+c+d)}$
  \item[E2] $\frac{(a+b)(b+d)}{(a+b+c+d)}$
  \item[E3] $\frac{(c+d)(a+c)}{(a+b+c+d)}$
  \item[E3] $\frac{(c+d)(b+d)}{(a+b+c+d)}$
  \end{itemize}

    $LL = G^2 = 2 (a \log (a/E1) + b \log (b/E2) +c \log(c/E3) + d \log(d/E4))$

\end{frame}


\begin{frame}{Collocations}
  Now we can not only extract frequent bigrams, but also we can find words that significantly associated with each other.

  We may be interested in collocations with some specific word.
\end{frame}


\begin{frame}
  \frametitle{tidylo by Julia Silge: weighted log odds}
  \begin{enumerate}
  \item Log odds ratio:
    $$
    O_1 = \frac{f_{(w,c1)}}{N_{c1}-f_{(w,c1)}}
    $$
    $$
    O_2 = \frac{f_{(w,c2)}}{N_{c2}-f_{(w,c2)}}
    $$
    $$
    LO = log \frac{O_1}{O_2}
    $$
  \item Weighted by uninformative Dirichlet prior:
    $$
    \delta =
    \frac{\frac{f_{(w,c1)}+\alpha_{(w,c1)}}{N_{c1}+\alpha_{c1}-f_{(w,c1)}-\alpha_{(w,c1)}}}{\frac{f_{(w,c2)}+\alpha_{(w,c2)}}{N_{c2}+\alpha_{c2}-f_{(w,c2)}-\alpha{(w,c2)}}}
    $$
  \end{enumerate}

  package tidylo in R
\end{frame}


\end{document}
