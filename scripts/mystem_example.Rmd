---
title: "practice2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Лемматизация

Лемматизация — приведение словоформ в тексте к начальным формам
(леммам). Позволяет абстрагироваться от словоизменения (падежных форм
существительных и прилагательных, временных форм глаголов и т.п.) и
сосредоточиться на лексических единицах текста.

Для лемматизации мы будем использовать mystem — свободно
распространяемую утилиту, разработанную в Яндексе. Mystem работает по
принципу фильтра: принимает на вход текст и возвращает текст в
лемматизированной форме и с грамматической информацией (в зависимости
от опций вызова). Вызвать утилиту можно из командной строки
операционной системы или непосредственно из R. 

Mystem уже установлен на сервере, но если вам потребуется, то нужно
будет установить его на компьютер.

Как скачать mystem?
1. https://yandex.ru/dev/mystem/
2. Скачиваем версию 3.1 для вашего устройства
3. После загрузки, переместите mystem в документы или туда, откуда вам удобнее будем его вызывать.

Вызовем mystem с помощью функции system2 (текст для лемматизации — в
колонке text, обязательно должна присутствовать опция stdout =
TRUE). Результат лемматизации сохраним в колонке lem.

```{r}
library(readr)
library(tidytext)
library(dplyr)
poems <- read_csv("poems_religion.csv", col_names=c('url_poem', 'author', 'url_author', 'name', 'poem'))


# посмотрим с маленьких примеров из 10 текстов
poems_test <- head(poems, 10)
# плюс надо обязательно заменить переносы строк на пробелы, иначе все сломается
library(stringr)
poems_test$poem <- str_replace_all(poems_test$poem, '\n', ' ')


library("stopwords")
rustop <- tibble(word = stopwords("ru"))

poems_test$lem <- system2("/home/alyona-pestova/Рабочий стол/CMAT/cmta2022/scripts/mystem", c("-d", "-l", "-c"), input = poems_test$poem, stdout = TRUE)

```

Теперь мы можем построить лемматизированный частотный список. Обратите
внимание, что в качестве колонки с исходным текстом мы берем теперь не
text, а lem, где находятся уже лемматизированные данные.

```{r}
sp.lemmas <- poems_test %>% unnest_tokens(word, lem)
```

**NB** Правильная последовательность операций: СНАЧАЛА лемматизация,
ПОТОМ токенизация. Поскольку в этом случае есть возможность применить
контекстое снятие омонимии, встроенное в mystem (опция "-d").

Вот у нас имеется готовая колонка с леммами. Можно строить частотный
список: 

```{r}
sp.freq <- sp.lemmas %>% count(word, sort=TRUE)
sp.freq
```

### Mystem: извлечение грамматической информации

Что еще можно сделать с помощью mystem?  Можно заглянуть в
документацию https://yandex.ru/dev/mystem/doc/ и извлечь не только
леммы, но и грамматическую информацию о слове (например части речи,
род). 

Для вывода грамматической информации добавим опции "-ig" в вызов
mystem и сохраним результат в колонке gram.

```{r}
poems_test$gram <- system2("/home/alyona-pestova/Рабочий стол/CMAT/cmta2022/scripts/mystem", c("-d", "-l", "-c", "-ig"), input = poems_test$poem, stdout = TRUE)

poems_test
```

В грамматической информации для нас могут быть интересны части речи и
другие граммемы: S - существительное, V — глагол, APRO -
местоимение-прилагательное и т.п. Расшифровку всех обозначений граммем
можно найти в документации mystem.

### Регулярные выражения

Что это? Это специальный язык для описания шаблонов строк, который
используется для поиска определенных строк, проверки их на
соответствие какому-либо шаблону и другой подобной работы. Регулярные
выражения — незаменимый инструмент для парсинга текстовых данных,
чистки текста в ходе препроцессинга и извлечения определенной
информации из текста.

Полезные ресурсы:
1. На английском https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/regex
2. На русском, структурированно и понятно http://website-lab.ru/article/regexp/shpargalka_po_regulyarnyim_vyirajeniyam/

Пример использования при чистке текста:

Избавляемся от пунктуации в текстах (используем функцию gsub из
базового R):

```{r}
poems_test$clean <- gsub("[[:punct:]]", " ", poems_test$poem) #избавляемся от пунктуации
```

Избавляемся от цифр (используем функцию str_replace_all, функционально
аналогичную gsub, но из пакета stringr):

```{r}
library(stringr)
poems_test$clean <- str_replace_all(poems_test$clean, "[0-9]+", " ")
```

### Извлечение лемм и частей речи из вывода mystem

Здесь мы извлечем в отдельную колонку части речи. А также избавимся от
всего лишнего в колонке lem. Для этого воспользуемся возможностью
передать в качестве токенизатора в unnest_tokens любую функцию. В
нашем случае это будет функция поиска по регулярному выражению
str_extract_all из пакета stringr.

Легко заметить, что в колонке gram части речи указаны после символа =
заглавными латинскими буквами. Извлечем леммы вместе с частью
речи. Для этого в параметре pattern опишем структуру токенов
(лемма=ЧАСТЬ_РЕЧИ) с помощью регулярного выражения. Все, что не
совпадает с этим шаблоном, будет просто проигнорировано.

```{r}
sp.gram <- poems_test %>% unnest_tokens(word, gram, token = stringr::str_extract_all, pattern="\\w+=[A-Z]+")
head(sp.gram)
```

Разберем состав регулярного выражения: 

* \\w → любая буква
* + → одно или более повторений предыдущего символа
* \\w+ → любая буква один или более раз
* = → просто символ “=”
* [A-Z] → любой символ из диапазона A-Z (любая заглавная латинская
  буква)
* [A-Z]+ → одна или более заглавных латинских букв
* "\\w+=[A-Z]+" → одна или более любых букв, за которыми следует
  символ “равно”, за которым следует одна или более заглавных
  латинских букв.
  
Теперь осталось только отделить части речи от лемм с помощью уже
известной нам функции tidyr::separate по символу "=".

```{r}
library(tidyr)
sp.lempos <- sp.gram %>%
    separate(word, c("lemma", "pos"), sep = "=") 
head(sp.lempos)
```

P.S. Для лемматизации и для определения частей речи можно использовать
пакет udpipe. Его преимущества: не требуется пост-обработка результата
лемматизации регулярными выражениями (получается сразу удобный
токенизированный дата-фрейм) с леммами, частями речи, синтаксической
информацией. Его недостатки: более медленная обработка, больше ошибок
при определении частей речи и грамматических форм слов.
